{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the eCOBIDAS documentation Motivations Poor methods and results description hinders the reproducibility and the replicability of research. It also makes it hard to compare new and old results and generally increases inefficiency in the research process. This project is built on the hope that improving methods and results reporting could improve our research. See here for more background information. Goals The short term goal of this project is to make the COBIDAS report (and other best practices for methods reporting) easier to use: we want to create a website with a clickable checklist that, at the end, automatically generates most of the method section of a (f)MRI / (i)EEG / MEG / PET paper. See here for more information on our vision and general goals. You can also go directly to one of the following section to see a breakdown of those goals and see some extensions we have in mind for the project. short term goals intermediate goals long term goals If you are interested by any of those, get in touch . Contributing to this project does not necessarily require super-advanced technical skills (except maybe a certain love for working with spreadsheet and wanting them to be super organized) :wink:. Project structure See here for more information about how this whole project is organized. Spreadsheets A lot of the work for this projects starts with some (usually pretty big) spreadsheets. See here for a description of their content and what the work involves. How to run the checklist See here for more information on how to work on the checklist on your own computer. References See here .","title":"Welcome"},{"location":"#welcome-to-the-ecobidas-documentation","text":"","title":"Welcome to the eCOBIDAS documentation"},{"location":"#motivations","text":"Poor methods and results description hinders the reproducibility and the replicability of research. It also makes it hard to compare new and old results and generally increases inefficiency in the research process. This project is built on the hope that improving methods and results reporting could improve our research. See here for more background information.","title":"Motivations"},{"location":"#goals","text":"The short term goal of this project is to make the COBIDAS report (and other best practices for methods reporting) easier to use: we want to create a website with a clickable checklist that, at the end, automatically generates most of the method section of a (f)MRI / (i)EEG / MEG / PET paper. See here for more information on our vision and general goals. You can also go directly to one of the following section to see a breakdown of those goals and see some extensions we have in mind for the project. short term goals intermediate goals long term goals If you are interested by any of those, get in touch . Contributing to this project does not necessarily require super-advanced technical skills (except maybe a certain love for working with spreadsheet and wanting them to be super organized) :wink:.","title":"Goals"},{"location":"#project-structure","text":"See here for more information about how this whole project is organized.","title":"Project structure"},{"location":"#spreadsheets","text":"A lot of the work for this projects starts with some (usually pretty big) spreadsheets. See here for a description of their content and what the work involves.","title":"Spreadsheets"},{"location":"#how-to-run-the-checklist","text":"See here for more information on how to work on the checklist on your own computer.","title":"How to run the checklist"},{"location":"#references","text":"See here .","title":"References"},{"location":"10-motivations/","text":"Project motivations Improve transparency and reproducibility In 2012, in a review of the methods of more than 200 fMRI papers, the author found that \" Although many journals urge authors to describe their methods to a level of detail such that independent investigators can fully reproduce their efforts, the results described here suggest that few studies meet this criterion. \" A few years ago, in order to improve reproducibility in f/MRI research, the Committee on Best Practices in Data Analysis and Sharing ( COBIDAS ) of OHBM released a report to promote best practices for methods and results reporting. This was recently followed by a similar initiative for EEG and MEG . Contrary to what the most optimistic people might have thought, these guidelines do not seem to have been widely adopted and anecdotal evidence ( see that twitter poll and thread ) suggests that, even among people who know about the report, few of them use it to write or review papers. A survey from ISMRM seems to suggest that the lack of method details is big problem in MRI research in general but also that there seems to be little knowledge of the existence of the COBIDAS report outside of some circles. One possible reason for this might be the unwieldy nature of the report. Anticipating this issue, the authors of the guidelines included a checklist that still ended up taking almost 30 of the 70 pages of the whole document. Anyone who has used this checklist tends to agree that it is a great resource but that it is a bit cumbersome to interpret and apply. So the goal of this project is to facilitate the use of this checklist and of similar guidelines. Reduce inefficiencies Currently, the research workflow around methods reporting has us do the same thing several times (very often manually) which leads to inefficiencies and multiplies the chances for human errors like forgetting to report some parameters or mis-reporting it. For example, a researcher would have to write or rewrite some aspects of the methods used when when preparing a pre-registration describing the planned study during the data curation process that usually involves adding metadata elements that relate to the details of the data acquisition, when actually working the methods and results section where we have to back and forth to the code we used to run the experiment and to the dataset to make sure important details are accurately reported, when sharing raw or derived data which also usually involves adding a minimum of methods-related metadata if the shared data is to be meaningfully reusable. Another source of inefficiency is the time lost trying to figure out: what the authors of a paper actually did when we would like to compare our results to theirs when reviewing papers what we actually did 6 months ago but forgot to make a note of it. So a potential side effect of using a checklist to systematically capture how data was acquired and analyzed would therefore be to make us more efficient. Having filled in a checklist once to generate a small set of files that capture the vast majority of our acquisition and analysis pipeline, we could easily share those files with our data, when submitting a paper or to generate a method section automatically.","title":"Motivations"},{"location":"10-motivations/#project-motivations","text":"","title":"Project motivations"},{"location":"10-motivations/#improve-transparency-and-reproducibility","text":"In 2012, in a review of the methods of more than 200 fMRI papers, the author found that \" Although many journals urge authors to describe their methods to a level of detail such that independent investigators can fully reproduce their efforts, the results described here suggest that few studies meet this criterion. \" A few years ago, in order to improve reproducibility in f/MRI research, the Committee on Best Practices in Data Analysis and Sharing ( COBIDAS ) of OHBM released a report to promote best practices for methods and results reporting. This was recently followed by a similar initiative for EEG and MEG . Contrary to what the most optimistic people might have thought, these guidelines do not seem to have been widely adopted and anecdotal evidence ( see that twitter poll and thread ) suggests that, even among people who know about the report, few of them use it to write or review papers. A survey from ISMRM seems to suggest that the lack of method details is big problem in MRI research in general but also that there seems to be little knowledge of the existence of the COBIDAS report outside of some circles. One possible reason for this might be the unwieldy nature of the report. Anticipating this issue, the authors of the guidelines included a checklist that still ended up taking almost 30 of the 70 pages of the whole document. Anyone who has used this checklist tends to agree that it is a great resource but that it is a bit cumbersome to interpret and apply. So the goal of this project is to facilitate the use of this checklist and of similar guidelines.","title":"Improve transparency and reproducibility"},{"location":"10-motivations/#reduce-inefficiencies","text":"Currently, the research workflow around methods reporting has us do the same thing several times (very often manually) which leads to inefficiencies and multiplies the chances for human errors like forgetting to report some parameters or mis-reporting it. For example, a researcher would have to write or rewrite some aspects of the methods used when when preparing a pre-registration describing the planned study during the data curation process that usually involves adding metadata elements that relate to the details of the data acquisition, when actually working the methods and results section where we have to back and forth to the code we used to run the experiment and to the dataset to make sure important details are accurately reported, when sharing raw or derived data which also usually involves adding a minimum of methods-related metadata if the shared data is to be meaningfully reusable. Another source of inefficiency is the time lost trying to figure out: what the authors of a paper actually did when we would like to compare our results to theirs when reviewing papers what we actually did 6 months ago but forgot to make a note of it. So a potential side effect of using a checklist to systematically capture how data was acquired and analyzed would therefore be to make us more efficient. Having filled in a checklist once to generate a small set of files that capture the vast majority of our acquisition and analysis pipeline, we could easily share those files with our data, when submitting a paper or to generate a method section automatically.","title":"Reduce inefficiencies"},{"location":"20-goals/","text":"Goals By turning recommendations and guidelines into a series of checklists hosted on a website, users could more rapidly click through it and provide more of the recommended information. This would generate a small text file that summarizes what option was chosen for each item of the checklist. This machine readable file could then be used to automatically generate part of the methods section of an article. But, if done right, this could also in the long term enhance the adoption of emerging neuroimaging standards (BIDS, fMRIprep, NIDM...), facilitate data sharing and pre-registration, help with peer-review... Vision We envision that using checklists to report methods and results can: provide comprehensive human and machine readable descriptions of the data collection and analysis pipelines used in published papers: in other words we want an app to help document pipelines to improve the reproducibility of our work and to reduce inefficiencies and frictions when trying to build on each other's work. facilitate the creation and preparation of pre-registration and registered reports by reminding of future analysis steps that we might otherwise overlook or forget about: in other words we want an app to help us think about and create pipelines before we start collecting data. help make peer-review more objective: we want an app to help us check pipelines. facilitate systematic literature reviews and meta-analyses (use the app to read pipelines) facilitate data sharing (use the app to standardize the report of information on a pipeline) Constraints The implementation of this project should remain flexible enough to: accommodate the inclusion of new items in the checklist as new methods mature (e.g. new multivariate analysis, high-resolution MRI...), easily fork the project and convert it to create a checklist-website for a different field. Milestones We are trying to divide our goals and milestones in 3 main categories: short term intermediate long term","title":"Goals"},{"location":"20-goals/#goals","text":"By turning recommendations and guidelines into a series of checklists hosted on a website, users could more rapidly click through it and provide more of the recommended information. This would generate a small text file that summarizes what option was chosen for each item of the checklist. This machine readable file could then be used to automatically generate part of the methods section of an article. But, if done right, this could also in the long term enhance the adoption of emerging neuroimaging standards (BIDS, fMRIprep, NIDM...), facilitate data sharing and pre-registration, help with peer-review...","title":"Goals"},{"location":"20-goals/#vision","text":"We envision that using checklists to report methods and results can: provide comprehensive human and machine readable descriptions of the data collection and analysis pipelines used in published papers: in other words we want an app to help document pipelines to improve the reproducibility of our work and to reduce inefficiencies and frictions when trying to build on each other's work. facilitate the creation and preparation of pre-registration and registered reports by reminding of future analysis steps that we might otherwise overlook or forget about: in other words we want an app to help us think about and create pipelines before we start collecting data. help make peer-review more objective: we want an app to help us check pipelines. facilitate systematic literature reviews and meta-analyses (use the app to read pipelines) facilitate data sharing (use the app to standardize the report of information on a pipeline)","title":"Vision"},{"location":"20-goals/#constraints","text":"The implementation of this project should remain flexible enough to: accommodate the inclusion of new items in the checklist as new methods mature (e.g. new multivariate analysis, high-resolution MRI...), easily fork the project and convert it to create a checklist-website for a different field.","title":"Constraints"},{"location":"20-goals/#milestones","text":"We are trying to divide our goals and milestones in 3 main categories: short term intermediate long term","title":"Milestones"},{"location":"21-short-term/","text":"Short term goals The short term goal of this project is to make the COBIDAS report (and similar guidelines) easier to use. We want to create a prototype website with a clickable checklist that, at the end, automatically generates most of the method section of a (f)MRI or (i)EEG / MEG or PET paper. So far the common short goals of all the versions of the app (for MRI, PET...) are: Create a set of tools and a proof of concept web-app that can: convert a set of spreadsheet of items into a schema that represents all from this schema generate a checklist to be clicked through by users, outputs a set of JSON-LD files once the user is done, generate a method section using these JSON-LD files and some boilerplate template of a method section where the content of the JSON-LD files could be reinjected. For the spreadsheets that represent the recommendation guidelines, the initial curation process must: identify high-priority items for each checklist, ensure that those high priority items has been properly atomized (meaning that it is only made of a single question) and curated (define an item name, a question, the type of response expected and an eventual list of response choices). MRI The current version of MRI prototype is inspired from Neurovault , so we would have to expand from there. The goal for the MRI app would to be able to describe a typical fMRI study with: a single functional task one anatomical scan using mass uni-variate analysis M/EEG The MRI version is currently ahead and the work done there can pave the way for the MEEG version. The MEEG COBIDAS guidelines have recently been published (see the preprint and webpage ). The main short term goals for the MEEG version are: Identify overlaps between the MEEG and the MRI spreadsheets and harmonize both versions by extracting the common parts into standalone spreadsheets: for example there could be one common spreadsheet for participant sample description. Consolidate the other items of the spreadsheet, as it is still missing a lot of information Identify high-priority items in the checklist (similar to Carp 2012 for fMRI, e.g. Luck & Gaspelin 2015 ) Positron emission tomography and Eyetracking Those 2 projects are more quite ahead already as they both started from fairly standardized spreadsheets. Both could benefit from a better definition of the response types and reponse options.","title":"Short term goals"},{"location":"21-short-term/#short-term-goals","text":"The short term goal of this project is to make the COBIDAS report (and similar guidelines) easier to use. We want to create a prototype website with a clickable checklist that, at the end, automatically generates most of the method section of a (f)MRI or (i)EEG / MEG or PET paper. So far the common short goals of all the versions of the app (for MRI, PET...) are: Create a set of tools and a proof of concept web-app that can: convert a set of spreadsheet of items into a schema that represents all from this schema generate a checklist to be clicked through by users, outputs a set of JSON-LD files once the user is done, generate a method section using these JSON-LD files and some boilerplate template of a method section where the content of the JSON-LD files could be reinjected. For the spreadsheets that represent the recommendation guidelines, the initial curation process must: identify high-priority items for each checklist, ensure that those high priority items has been properly atomized (meaning that it is only made of a single question) and curated (define an item name, a question, the type of response expected and an eventual list of response choices).","title":"Short term goals"},{"location":"21-short-term/#mri","text":"The current version of MRI prototype is inspired from Neurovault , so we would have to expand from there. The goal for the MRI app would to be able to describe a typical fMRI study with: a single functional task one anatomical scan using mass uni-variate analysis","title":"MRI"},{"location":"21-short-term/#meeg","text":"The MRI version is currently ahead and the work done there can pave the way for the MEEG version. The MEEG COBIDAS guidelines have recently been published (see the preprint and webpage ). The main short term goals for the MEEG version are: Identify overlaps between the MEEG and the MRI spreadsheets and harmonize both versions by extracting the common parts into standalone spreadsheets: for example there could be one common spreadsheet for participant sample description. Consolidate the other items of the spreadsheet, as it is still missing a lot of information Identify high-priority items in the checklist (similar to Carp 2012 for fMRI, e.g. Luck & Gaspelin 2015 )","title":"M/EEG"},{"location":"21-short-term/#positron-emission-tomography-and-eyetracking","text":"Those 2 projects are more quite ahead already as they both started from fairly standardized spreadsheets. Both could benefit from a better definition of the response types and reponse options.","title":"Positron emission tomography and Eyetracking"},{"location":"22-mid-term/","text":"Intermediate goals Once we have a prototype for a checklist we want to optimize them to make that they are comprehensive and cover all the items of the guidelines and to minimize the amount of time the user has to interact with the checklist. Improve user-friendliness In line with the idea of trying to minimize how much time the users have to spend using the app, we want to pre-select response choices to some items depending on the software users report having used in their analysis. For example, if SPM was used to do slice timing correction, there is no reason to ask users what type of interpolation was used as SPM does not give users the possibility to choose that parameter. Finding out what are the choice options for each item for the neuroimaging software used could help users navigate the app faster. Another potential way to improve users experience is to only expose users to items that are relevant to them. For example, we want to make sure that the items related \"arterial spin labelling\" are displayed only if the user mentioned they used this technique in a previous section of the checklist. Improving the wording The questions of the checklist must be as unambiguous as possible. This should be improved through early user feedback. Extended checklists Right now, several of the prototypes contains only a subset of all the questions from the reports they came from. For example, the MRI checklist only contain the items corresponding to the metadata of a collection of results uploaded on Neurovault . In the near future, we want to be able to extend those checklists so they include all the items listed in their guidelines. Links to standardized pipelines Data processed with some standardized pipelines (like fMRIprep ) could facilitate filling in the checklist: ticking the box corresponding to that pipeline would automatically populate all the relevant fields in the COBIDAS-json file. Standardize terminology Ensure that the across apps the same terminology is used by trying to rely on the BIDS terminology with reference to other lexicons or ontology.","title":"Mid term goals"},{"location":"22-mid-term/#intermediate-goals","text":"Once we have a prototype for a checklist we want to optimize them to make that they are comprehensive and cover all the items of the guidelines and to minimize the amount of time the user has to interact with the checklist.","title":"Intermediate goals"},{"location":"22-mid-term/#improve-user-friendliness","text":"In line with the idea of trying to minimize how much time the users have to spend using the app, we want to pre-select response choices to some items depending on the software users report having used in their analysis. For example, if SPM was used to do slice timing correction, there is no reason to ask users what type of interpolation was used as SPM does not give users the possibility to choose that parameter. Finding out what are the choice options for each item for the neuroimaging software used could help users navigate the app faster. Another potential way to improve users experience is to only expose users to items that are relevant to them. For example, we want to make sure that the items related \"arterial spin labelling\" are displayed only if the user mentioned they used this technique in a previous section of the checklist.","title":"Improve user-friendliness"},{"location":"22-mid-term/#improving-the-wording","text":"The questions of the checklist must be as unambiguous as possible. This should be improved through early user feedback.","title":"Improving the wording"},{"location":"22-mid-term/#extended-checklists","text":"Right now, several of the prototypes contains only a subset of all the questions from the reports they came from. For example, the MRI checklist only contain the items corresponding to the metadata of a collection of results uploaded on Neurovault . In the near future, we want to be able to extend those checklists so they include all the items listed in their guidelines.","title":"Extended checklists"},{"location":"22-mid-term/#links-to-standardized-pipelines","text":"Data processed with some standardized pipelines (like fMRIprep ) could facilitate filling in the checklist: ticking the box corresponding to that pipeline would automatically populate all the relevant fields in the COBIDAS-json file.","title":"Links to standardized pipelines"},{"location":"22-mid-term/#standardize-terminology","text":"Ensure that the across apps the same terminology is used by trying to rely on the BIDS terminology with reference to other lexicons or ontology.","title":"Standardize terminology"},{"location":"23-long-term/","text":"\"pie in the sky\" goals On the long term, we want those web-app to integrate better with other data standards and softwares to further reduce how much manual entry is required from the user. Similarly we want to broaden the use-cases for the app. Improve user-friendliness Improving default answers If some data is gathered about the content of the method section of a sample of articles in the litterature (see Carp, 2012 or the eyetracking guidelines ), it should be possible to create or better organize list of response choices. It will take less time for users to tick a box rather than type something. Similarly we could decide to make the most common choices or the \"better\" option more prominent in a list of response options. Adding a help section for each item The content of some item might be quite obscure for some users, so embedding some help about each item would be a desirable goal. Integration with the main neuroimaging software Plugins A side branch of this project includes developing plugins for the main neuroimaging software toolboxes (SPM, FSL, AFNI). Such a plugin would receive processing batch files as input (e.g matlabbatch.mat for SPM or design.fsf for FSL) and would create output files (e.g. in json format) readable by the checklist website, which in turn would be able to display the neuroimaging processing pipeline and/or the methods section for an article with the appropriate references (akin to what fMRIprep already does). In a similar way, FSL already outputs methods description for every preprocessing or statistical analysis that can be used to write the methods section of a paper as explained by Jeanette Mumford in this video . Integration with BIDS and NIDM The process of filling in an online checklist can be simplified if the required information can be directly accessed from the metadata in some of the existing standards for data and results like: the brain imaging data structure ( BIDS ) used for that study, the NIDM results of any mass-univariate analysis performed for this study. BIDS is an emerging standard to organize neuroimaging data (MRI, fMRI, EEG...) and its associated metadata regarding acquisition (like repetition time, echo time, slice order...), experimental design (number of subjects, conditions, stimulus onsets...). If a data set is organized following the BIDS standard, it is possible to query its content using pybids or bids-matlab and even write part of the method section relative to the acquisition usig the reports module of those pacakges (see for example pybids reports ). NIDM results is a way to package mass-univariate fMRI results in a software independent way. This way it can for example facilitate reading FSL results through SPM or facilitate data sharing (like NIDM results is supported by the data sharing platform Neurovault and this greatly facilitate uploading your results there). NIDM contains many metadata with a correspondence in the COBIDAS checklist (see here ). For BIDS and NIDM results, machine reading through a dataset organized as BIDS and NIDM results could automatically fill part of the COBIDAS checklist. Conversely it is possible to help users create some files that make up a BIDS dataset by having them fill in part of the checklist. An implementation of this, will most likely rely on the respective schema of those resources: BIDS schema . NIDM schema: link missing Integration with existing ontologies To automate the creation certain aspects we could try to rely on some existing ontologies that create list of term definitions to generate or enrich the list of response choices to some items. Examples of this could rely on scicrunch and the NIDM-terms or the Research resource IDentifiers . Link to pipeline-creating tools Though more challenging, it could be imagined that pipeline-creating-tools like ( nipype , PORCUPINE , GIRAFFE ) could also generate json files to be used for method section generation. Facilitate data sharing If the output created by the checklist app is accepted by neuroimaging databases (e.g. Neurovault ) as metadata input, this could reduces friction for data sharing. Users wouldn't have to fill several times the same information: when pre-registering, when writing their methods, when submitting to a journal (if it has a checklist of its own), when sharing data. For example the metadata fields from neurovault are already present in the COBIDAS checklist. Link to journal specific checklists As some journals start having submission-checklists (e.g eLife, nature research journals ...), filling in the a checklist only once could reduce 'the submission paperwork' by writing part of the method section of the article AND generating the appropriate submission-checklist for a given journal. Reviewers could also use the website to systematically cross-check that all the required methods and results information are present for a given paper. A tool to compare different outputs from the web app could help editors visualize agreement across reviewers' evaluation of a paper. If the process is made sufficiently seamless, this could in return potentially incentivize more journals to adopt submission-checklists.","title":"Long term goals"},{"location":"23-long-term/#pie-in-the-sky-goals","text":"On the long term, we want those web-app to integrate better with other data standards and softwares to further reduce how much manual entry is required from the user. Similarly we want to broaden the use-cases for the app.","title":"\"pie in the sky\" goals"},{"location":"23-long-term/#improve-user-friendliness","text":"","title":"Improve user-friendliness"},{"location":"23-long-term/#improving-default-answers","text":"If some data is gathered about the content of the method section of a sample of articles in the litterature (see Carp, 2012 or the eyetracking guidelines ), it should be possible to create or better organize list of response choices. It will take less time for users to tick a box rather than type something. Similarly we could decide to make the most common choices or the \"better\" option more prominent in a list of response options.","title":"Improving default answers"},{"location":"23-long-term/#adding-a-help-section-for-each-item","text":"The content of some item might be quite obscure for some users, so embedding some help about each item would be a desirable goal.","title":"Adding a help section for each item"},{"location":"23-long-term/#integration-with-the-main-neuroimaging-software","text":"","title":"Integration with the main neuroimaging software"},{"location":"23-long-term/#plugins","text":"A side branch of this project includes developing plugins for the main neuroimaging software toolboxes (SPM, FSL, AFNI). Such a plugin would receive processing batch files as input (e.g matlabbatch.mat for SPM or design.fsf for FSL) and would create output files (e.g. in json format) readable by the checklist website, which in turn would be able to display the neuroimaging processing pipeline and/or the methods section for an article with the appropriate references (akin to what fMRIprep already does). In a similar way, FSL already outputs methods description for every preprocessing or statistical analysis that can be used to write the methods section of a paper as explained by Jeanette Mumford in this video .","title":"Plugins"},{"location":"23-long-term/#integration-with-bids-and-nidm","text":"The process of filling in an online checklist can be simplified if the required information can be directly accessed from the metadata in some of the existing standards for data and results like: the brain imaging data structure ( BIDS ) used for that study, the NIDM results of any mass-univariate analysis performed for this study. BIDS is an emerging standard to organize neuroimaging data (MRI, fMRI, EEG...) and its associated metadata regarding acquisition (like repetition time, echo time, slice order...), experimental design (number of subjects, conditions, stimulus onsets...). If a data set is organized following the BIDS standard, it is possible to query its content using pybids or bids-matlab and even write part of the method section relative to the acquisition usig the reports module of those pacakges (see for example pybids reports ). NIDM results is a way to package mass-univariate fMRI results in a software independent way. This way it can for example facilitate reading FSL results through SPM or facilitate data sharing (like NIDM results is supported by the data sharing platform Neurovault and this greatly facilitate uploading your results there). NIDM contains many metadata with a correspondence in the COBIDAS checklist (see here ). For BIDS and NIDM results, machine reading through a dataset organized as BIDS and NIDM results could automatically fill part of the COBIDAS checklist. Conversely it is possible to help users create some files that make up a BIDS dataset by having them fill in part of the checklist. An implementation of this, will most likely rely on the respective schema of those resources: BIDS schema . NIDM schema: link missing","title":"Integration with BIDS and NIDM"},{"location":"23-long-term/#integration-with-existing-ontologies","text":"To automate the creation certain aspects we could try to rely on some existing ontologies that create list of term definitions to generate or enrich the list of response choices to some items. Examples of this could rely on scicrunch and the NIDM-terms or the Research resource IDentifiers .","title":"Integration with existing ontologies"},{"location":"23-long-term/#link-to-pipeline-creating-tools","text":"Though more challenging, it could be imagined that pipeline-creating-tools like ( nipype , PORCUPINE , GIRAFFE ) could also generate json files to be used for method section generation.","title":"Link to pipeline-creating tools"},{"location":"23-long-term/#facilitate-data-sharing","text":"If the output created by the checklist app is accepted by neuroimaging databases (e.g. Neurovault ) as metadata input, this could reduces friction for data sharing. Users wouldn't have to fill several times the same information: when pre-registering, when writing their methods, when submitting to a journal (if it has a checklist of its own), when sharing data. For example the metadata fields from neurovault are already present in the COBIDAS checklist.","title":"Facilitate data sharing"},{"location":"23-long-term/#link-to-journal-specific-checklists","text":"As some journals start having submission-checklists (e.g eLife, nature research journals ...), filling in the a checklist only once could reduce 'the submission paperwork' by writing part of the method section of the article AND generating the appropriate submission-checklist for a given journal. Reviewers could also use the website to systematically cross-check that all the required methods and results information are present for a given paper. A tool to compare different outputs from the web app could help editors visualize agreement across reviewers' evaluation of a paper. If the process is made sufficiently seamless, this could in return potentially incentivize more journals to adopt submission-checklists.","title":"Link to journal specific checklists"},{"location":"30-general-organization/","text":"General organization The general workflow of this project is the following: turning the recommendation guidelines into spreadsheets turning the spreadsheets into a \"schema\" representation using a \"front-end\" user-interface that will read those schema and serve a web-app to the user. To execute that work, this project is organized around several \"repositories\": the eCOBIDAS repository centralises most of the information and workflow to convert the guidelines into a checklist webapp, the Reproschema user interface contains the \"front-end\" code of the user interface to render the checklist webapp, the ReproSchema repository contains the formal \"definition\" of the terms used to describe the content of the checklist as a schema, the repositories containing a) an instance of the user interface and b) a schema to serve a specific checklist : the one for the MRI version based of the Neurovault metadata \"checklist\" hosted on the OHBM Github organization and that serves this checklist , the one for the PET imaging version that serves this checklist , the google drive where we work synchronously on the spreadsheets , an associated zotero library to keep track of references related to this project, a project on the open-science framework that allows to \"connect\" all those elements together in one place. the eCOBIDAS repository This repository hosts the workflow that will turn the reports published by the Committee on Best Practices in Data Analysis and Sharing (COBIDAS) of the organization for human brain mapping (OHBM) into a checklists for improving methods and results reporting in (f)MRI, (i)EEG, MEG. By extension, this workflow can also be used on other types of guidelines (like the ones for PET imaging and eyetracking). . \u251c\u2500\u2500 .github <-- continuous integration \"scripts\" \u251c\u2500\u2500 activities <-- schema of the different \"sections\"of the checklistss with their items \u251c\u2500\u2500 communication <-- abstracts and presentations about the project \u251c\u2500\u2500 docs <-- content of the documentation \u251c\u2500\u2500 inputs <-- checklists spreadsheets as CSV files, boilerplate for method section generation \u251c\u2500\u2500 protocols <-- schema for the checklists putting together several \"sections\" together \u251c\u2500\u2500 response_options <-- contains the pre-set list of response options to some checklist items \u251c\u2500\u2500 schema <-- obsolete: ignore this \u251c\u2500\u2500 scripts <-- python scripts to convert the CSV spreadsheets into schemas \u2514\u2500\u2500 tests <-- python script to test that the schema files are valid JSON-LD Spreadsheet content and organization The first step of the workflow involves taking the recommendation guidelines and converting that into a spreadsheet that contains all the items of the future checklist. This step is by far the most labor intensive and has its dedicated page in the documentation Converting the spreadsheet into a schema Most of that is covered in the section on how the checklist is rendered and in the README in the scripts folder. How is the Reproschema organized The first step of the workflow involves taking a spreadsheet that contains all the items of the checklist and turning that into a representation that can efficiently link the metadata about each item to the data imputed by the user. We are using the ReproSchema initiative from ReproNim to do this. Basically, it means turning your 'dumb' spreadsheet into an equivalent but 'smarter' representation of it: a bunch of hierarchically organized json files that link to each other. On top of the inherent advantages of this schema representation: its use simplifies the rendering of the checklist by using the reproschema-ui made for it, this representation allows specification of user interface option that can simplify the user experience: it allows us to specify the conditions that will make certain items visible or not and thus will prevent users to be presented with items that are not relevant to them (e.g answer PET related when they have only run an fMRI study). The reproschema is organized in a hierarchical manner with several levels, the main ones being The lowest level is the item level where there is one question for each item with an expected format for the user interface: is this yes / no question (boolean), a multiple choice, a float or an integer... The second level is the activity level that contains a set of items. In the original repronim project this would constitute usually a questionnaire: like all the items of the Edinburgh handedness inventory would constitute one activity. In our case, we are using it to define to break a checklist into sub-sections of a method section like preprocessing, design, participants... The highest level is the protocol level that contains a set of activities. At the moment this level is under-used in our checklist but could be used to define activity sets for different use case: fMRI, MEEG, pre-registration... If you want to know more about Reproschema, we suggest you have look at the documentation main documentation FAQ We are also trying to extend the content of the documentation of the reproschema. You can keep track of this in this pull request and here on github","title":"General organization"},{"location":"30-general-organization/#general-organization","text":"The general workflow of this project is the following: turning the recommendation guidelines into spreadsheets turning the spreadsheets into a \"schema\" representation using a \"front-end\" user-interface that will read those schema and serve a web-app to the user. To execute that work, this project is organized around several \"repositories\": the eCOBIDAS repository centralises most of the information and workflow to convert the guidelines into a checklist webapp, the Reproschema user interface contains the \"front-end\" code of the user interface to render the checklist webapp, the ReproSchema repository contains the formal \"definition\" of the terms used to describe the content of the checklist as a schema, the repositories containing a) an instance of the user interface and b) a schema to serve a specific checklist : the one for the MRI version based of the Neurovault metadata \"checklist\" hosted on the OHBM Github organization and that serves this checklist , the one for the PET imaging version that serves this checklist , the google drive where we work synchronously on the spreadsheets , an associated zotero library to keep track of references related to this project, a project on the open-science framework that allows to \"connect\" all those elements together in one place.","title":"General organization"},{"location":"30-general-organization/#the-ecobidas-repository","text":"This repository hosts the workflow that will turn the reports published by the Committee on Best Practices in Data Analysis and Sharing (COBIDAS) of the organization for human brain mapping (OHBM) into a checklists for improving methods and results reporting in (f)MRI, (i)EEG, MEG. By extension, this workflow can also be used on other types of guidelines (like the ones for PET imaging and eyetracking). . \u251c\u2500\u2500 .github <-- continuous integration \"scripts\" \u251c\u2500\u2500 activities <-- schema of the different \"sections\"of the checklistss with their items \u251c\u2500\u2500 communication <-- abstracts and presentations about the project \u251c\u2500\u2500 docs <-- content of the documentation \u251c\u2500\u2500 inputs <-- checklists spreadsheets as CSV files, boilerplate for method section generation \u251c\u2500\u2500 protocols <-- schema for the checklists putting together several \"sections\" together \u251c\u2500\u2500 response_options <-- contains the pre-set list of response options to some checklist items \u251c\u2500\u2500 schema <-- obsolete: ignore this \u251c\u2500\u2500 scripts <-- python scripts to convert the CSV spreadsheets into schemas \u2514\u2500\u2500 tests <-- python script to test that the schema files are valid JSON-LD","title":"the eCOBIDAS repository"},{"location":"30-general-organization/#spreadsheet-content-and-organization","text":"The first step of the workflow involves taking the recommendation guidelines and converting that into a spreadsheet that contains all the items of the future checklist. This step is by far the most labor intensive and has its dedicated page in the documentation","title":"Spreadsheet content and organization"},{"location":"30-general-organization/#converting-the-spreadsheet-into-a-schema","text":"Most of that is covered in the section on how the checklist is rendered and in the README in the scripts folder.","title":"Converting the spreadsheet into a schema"},{"location":"30-general-organization/#how-is-the-reproschema-organized","text":"The first step of the workflow involves taking a spreadsheet that contains all the items of the checklist and turning that into a representation that can efficiently link the metadata about each item to the data imputed by the user. We are using the ReproSchema initiative from ReproNim to do this. Basically, it means turning your 'dumb' spreadsheet into an equivalent but 'smarter' representation of it: a bunch of hierarchically organized json files that link to each other. On top of the inherent advantages of this schema representation: its use simplifies the rendering of the checklist by using the reproschema-ui made for it, this representation allows specification of user interface option that can simplify the user experience: it allows us to specify the conditions that will make certain items visible or not and thus will prevent users to be presented with items that are not relevant to them (e.g answer PET related when they have only run an fMRI study). The reproschema is organized in a hierarchical manner with several levels, the main ones being The lowest level is the item level where there is one question for each item with an expected format for the user interface: is this yes / no question (boolean), a multiple choice, a float or an integer... The second level is the activity level that contains a set of items. In the original repronim project this would constitute usually a questionnaire: like all the items of the Edinburgh handedness inventory would constitute one activity. In our case, we are using it to define to break a checklist into sub-sections of a method section like preprocessing, design, participants... The highest level is the protocol level that contains a set of activities. At the moment this level is under-used in our checklist but could be used to define activity sets for different use case: fMRI, MEEG, pre-registration... If you want to know more about Reproschema, we suggest you have look at the documentation main documentation FAQ We are also trying to extend the content of the documentation of the reproschema. You can keep track of this in this pull request and here on github","title":"How is the Reproschema organized"},{"location":"40-spreadsheets/","text":"Spreadsheets The spreadsheets that allow us to generate the different checklists are hosted on this google drive folder and we try to keep a back-up in the csv folder . The neurovault spreadsheet is here The PET spreadsheet is here The eyetracker spreadsheet is here The MRI spreadsheet is here The M/EEG spreadsheet is here The download_csv.sh bash script will directly download those spreadsheets as csv files into the inputs/csv folder To do Here is a short list of the different things to keep in mind when working on one of the spreadsheet. Each line must correspond to one checklist item that must have only one unambiguous item with an associated question. Any item that opens the possibility of a response of the form: If A was used, then list the parameters B, C, D Then it must be broken down into several questions: 1. Was A used? 2. If so, what parameter was used for B? 3. What parameter was used for C? ... For each item: make sure it has a name, preferred label, description: for some of those a formula in the spreadsheet should automatically take care of that make sure that there is a clear specific and unambiguous question associated to this item identify the response type expected create a response choice list where needed mark the item as high-priority to be in the next release of the app. assess whether there is way to not expose users to that item (or restrict list of the response choices for that item) if it is not relevant to their use-case Working with the spreadsheets This describes some rules and tips when working with the spreadsheets. Style guide Where relevant we try to use snake_case and stick to lower case. Hidden columns If some columns do not appear, it is possible that they have been hidden by someone else. You will simply have to click on the double black arrow at the limit between columns to display them back. Formatting The MRI spreadsheet has some conditional formatting implemented so some cells will appear red when there is an error to be fixed (or if the cell is empty and should be filled). Some items that require more work might be manually highlighted in orange. Filtering If you want to only see certain rows, it is better to filter them rather than hide them. The column headers can be used to filter which item to display: for example, the activity_name column can be filtered using the arrow in the top cell in order to see only the items corresponding to one or more main sections (e.g., \"Acquisition\", \"Experimental design\", \"Preprocessing\", etc.). Formulas and automation There is a certain level of automation built into those spreadsheets to fill in certain columns. For example, the preferred label of an item is generated from the item name by removing any underscore. Given that some of the guidelines used as source material often had several sub and sub-sub-sections organized in a hierarchical fashion, this was in some cases used to generate the item names. Below is an example how the item name for the MRI spreadsheet is determined by the content of the right-most non-empty column on the lef of the item column. item_name Imaging type imaging_type Essential imaging parameters All acquisitions voxel dimension voxel_dimension Essential imaging parameters Slice timing slice_timing Spreadsheet content Here follows a description of the columns' content. Content common to all spreadsheets The description of the columns common to all spreadsheet is described in the data dictionary in the inputs folder . Each column is described by an element in the JSON data dictionarry. \"column_name\": { \"LongName\": \"\", \"VariableName\": \"name of the corresponding variable, if relevant, in the conversion scripts\", \"Description\": \"\", \"Levels\": \"describes the different possibilities in this column\" } Extra columns Software defaults columns ( spm_default , fsl_default ) refer to the default value used for this item by a given software. Integration with other sources: Brain Imaging Data Structure dataset: the bids_* columns denote if information about this item can be found in a BIDS dataset and if so where. nidm_results : mention where information this item can be found in an NIDM results package neurovault collection: refers to the name of this item in a Neurovault collection Meta-analysis: These columns denote whether or not each item could be important to evaluate studies for a meta-analysis. use_case_meta-analysis yes, include for meta-analyses no, exclude for meta-analyses maybe - meta-analysis_comment Percent of studies reporting the item Comparison to Carp 2012 Some columns list the percentage of studies that reported each item. For the MRI spreadsheet, this is taken from Carp, 2012 . in_Carp2012 : name of the item in that paper percent_of_studies : percent of studies reporting the item. If the number is in bold , it was approximately extracted from one of the figures of the paper. (because it was not reported in the text of the article). percent_of_studies_anat : same as above but for the anatomical data The Eyetracking spreadsheet percent_reported column also has numbers on the frequency at which items are present in the literature (see the preprint )","title":"Working with the spreadhseets"},{"location":"40-spreadsheets/#spreadsheets","text":"The spreadsheets that allow us to generate the different checklists are hosted on this google drive folder and we try to keep a back-up in the csv folder . The neurovault spreadsheet is here The PET spreadsheet is here The eyetracker spreadsheet is here The MRI spreadsheet is here The M/EEG spreadsheet is here The download_csv.sh bash script will directly download those spreadsheets as csv files into the inputs/csv folder","title":"Spreadsheets"},{"location":"40-spreadsheets/#to-do","text":"Here is a short list of the different things to keep in mind when working on one of the spreadsheet. Each line must correspond to one checklist item that must have only one unambiguous item with an associated question. Any item that opens the possibility of a response of the form: If A was used, then list the parameters B, C, D Then it must be broken down into several questions: 1. Was A used? 2. If so, what parameter was used for B? 3. What parameter was used for C? ... For each item: make sure it has a name, preferred label, description: for some of those a formula in the spreadsheet should automatically take care of that make sure that there is a clear specific and unambiguous question associated to this item identify the response type expected create a response choice list where needed mark the item as high-priority to be in the next release of the app. assess whether there is way to not expose users to that item (or restrict list of the response choices for that item) if it is not relevant to their use-case","title":"To do"},{"location":"40-spreadsheets/#working-with-the-spreadsheets","text":"This describes some rules and tips when working with the spreadsheets.","title":"Working with the spreadsheets"},{"location":"40-spreadsheets/#style-guide","text":"Where relevant we try to use snake_case and stick to lower case.","title":"Style guide"},{"location":"40-spreadsheets/#hidden-columns","text":"If some columns do not appear, it is possible that they have been hidden by someone else. You will simply have to click on the double black arrow at the limit between columns to display them back.","title":"Hidden columns"},{"location":"40-spreadsheets/#formatting","text":"The MRI spreadsheet has some conditional formatting implemented so some cells will appear red when there is an error to be fixed (or if the cell is empty and should be filled). Some items that require more work might be manually highlighted in orange.","title":"Formatting"},{"location":"40-spreadsheets/#filtering","text":"If you want to only see certain rows, it is better to filter them rather than hide them. The column headers can be used to filter which item to display: for example, the activity_name column can be filtered using the arrow in the top cell in order to see only the items corresponding to one or more main sections (e.g., \"Acquisition\", \"Experimental design\", \"Preprocessing\", etc.).","title":"Filtering"},{"location":"40-spreadsheets/#formulas-and-automation","text":"There is a certain level of automation built into those spreadsheets to fill in certain columns. For example, the preferred label of an item is generated from the item name by removing any underscore. Given that some of the guidelines used as source material often had several sub and sub-sub-sections organized in a hierarchical fashion, this was in some cases used to generate the item names. Below is an example how the item name for the MRI spreadsheet is determined by the content of the right-most non-empty column on the lef of the item column. item_name Imaging type imaging_type Essential imaging parameters All acquisitions voxel dimension voxel_dimension Essential imaging parameters Slice timing slice_timing","title":"Formulas and automation"},{"location":"40-spreadsheets/#spreadsheet-content","text":"Here follows a description of the columns' content.","title":"Spreadsheet content"},{"location":"40-spreadsheets/#content-common-to-all-spreadsheets","text":"The description of the columns common to all spreadsheet is described in the data dictionary in the inputs folder . Each column is described by an element in the JSON data dictionarry. \"column_name\": { \"LongName\": \"\", \"VariableName\": \"name of the corresponding variable, if relevant, in the conversion scripts\", \"Description\": \"\", \"Levels\": \"describes the different possibilities in this column\" }","title":"Content common to all spreadsheets"},{"location":"40-spreadsheets/#extra-columns","text":"Software defaults columns ( spm_default , fsl_default ) refer to the default value used for this item by a given software. Integration with other sources: Brain Imaging Data Structure dataset: the bids_* columns denote if information about this item can be found in a BIDS dataset and if so where. nidm_results : mention where information this item can be found in an NIDM results package neurovault collection: refers to the name of this item in a Neurovault collection Meta-analysis: These columns denote whether or not each item could be important to evaluate studies for a meta-analysis. use_case_meta-analysis yes, include for meta-analyses no, exclude for meta-analyses maybe - meta-analysis_comment","title":"Extra columns"},{"location":"40-spreadsheets/#percent-of-studies-reporting-the-item","text":"Comparison to Carp 2012 Some columns list the percentage of studies that reported each item. For the MRI spreadsheet, this is taken from Carp, 2012 . in_Carp2012 : name of the item in that paper percent_of_studies : percent of studies reporting the item. If the number is in bold , it was approximately extracted from one of the figures of the paper. (because it was not reported in the text of the article). percent_of_studies_anat : same as above but for the anatomical data The Eyetracking spreadsheet percent_reported column also has numbers on the frequency at which items are present in the literature (see the preprint )","title":"Percent of studies reporting the item"},{"location":"50-how-to-render-the-checklist/","text":"How the checklist is rendered This part gets a bit more \"techy\" but we will do our best to guide you through it. First make sure you are familiar with the structure of this project by reading the general organization documentation. Turn the spreadsheet into the schema This part is done by a bit of python code and you can find more information in the README in the scripts folder. Making the new schema available to the UI If the previous step went smoothly you now need to make the newly created files available on your remote repository on github so that the user-interface can 'see' them. This you can do by committing the files newly created on your local repository and pushing them the remote. For example if the schema is hosted on the neurovault branch of your origin remote repository, the following should do the trick (assuming that you are already on the neurovault branch of your local repo): git add --all git commit -m 'your commit message' git push origin neurovault Rendering the checklist There are 2 ways to visualize the checklist you have created. Using the online user interface You can point the already existing user-interface to the schema of the protocol or the activity you have just created. If you want to visualize an activity on its own, you can use the reproschema-ui . To do that you can point the UI to the raw content of this activity. To get access to the raw content of an activity you must click on the Raw button on github once you have opened its page, see for example the PHQ-9 acvitiy here . This will open this URL: https://raw.githubusercontent.com/ReproNim/reproschema-library/master/activities/PHQ-9/PHQ9_schema . You can then pass the the URL of raw content to the UI using the following template URL: https://www.repronim.org/reproschema-ui/#/activities/0?url=url-to-activity-schema To view a protocol, you can also use the reproschema-ui with the following template URL: https://www.repronim.org/reproschema-ui/#/?url=url-to-protocol-schema Serving the app locally If you want to use serve the app locally on your computer, you will need to install node.js (the \"backend\" version of Javascript). A good way to do this is to use node version manager: some installation instructions are available on the user-interface repository . Make sure that that you have set the UI correctly so it will read the schema from the right repository. This can be be set by modifying its reproschema-ui/src/config.js file (e.g see here ). Modify the githubSrc so that it points to the URL of the schema of your protocol: module.exports = { /* eslint-disable */ githubSrc: \"https://raw.githubusercontent.com/your-gtihub-account/your-repository/branch-name/folder/protcol_schema_filename\", banner: \"This is a test\", assetsPublicPath: \"/your-repository/\", backendServer: null, consent: true, }; Once you have done that you can launch the app with: npm install # Install the Javascript dependencies npm run serve # Run the development server locally Open your browser and go to localhost:8080","title":"Viewing the checklist"},{"location":"50-how-to-render-the-checklist/#how-the-checklist-is-rendered","text":"This part gets a bit more \"techy\" but we will do our best to guide you through it. First make sure you are familiar with the structure of this project by reading the general organization documentation.","title":"How the checklist is rendered"},{"location":"50-how-to-render-the-checklist/#turn-the-spreadsheet-into-the-schema","text":"This part is done by a bit of python code and you can find more information in the README in the scripts folder.","title":"Turn the spreadsheet into the schema"},{"location":"50-how-to-render-the-checklist/#making-the-new-schema-available-to-the-ui","text":"If the previous step went smoothly you now need to make the newly created files available on your remote repository on github so that the user-interface can 'see' them. This you can do by committing the files newly created on your local repository and pushing them the remote. For example if the schema is hosted on the neurovault branch of your origin remote repository, the following should do the trick (assuming that you are already on the neurovault branch of your local repo): git add --all git commit -m 'your commit message' git push origin neurovault","title":"Making the new schema available to the UI"},{"location":"50-how-to-render-the-checklist/#rendering-the-checklist","text":"There are 2 ways to visualize the checklist you have created.","title":"Rendering the checklist"},{"location":"50-how-to-render-the-checklist/#using-the-online-user-interface","text":"You can point the already existing user-interface to the schema of the protocol or the activity you have just created. If you want to visualize an activity on its own, you can use the reproschema-ui . To do that you can point the UI to the raw content of this activity. To get access to the raw content of an activity you must click on the Raw button on github once you have opened its page, see for example the PHQ-9 acvitiy here . This will open this URL: https://raw.githubusercontent.com/ReproNim/reproschema-library/master/activities/PHQ-9/PHQ9_schema . You can then pass the the URL of raw content to the UI using the following template URL: https://www.repronim.org/reproschema-ui/#/activities/0?url=url-to-activity-schema To view a protocol, you can also use the reproschema-ui with the following template URL: https://www.repronim.org/reproschema-ui/#/?url=url-to-protocol-schema","title":"Using the online user interface"},{"location":"50-how-to-render-the-checklist/#serving-the-app-locally","text":"If you want to use serve the app locally on your computer, you will need to install node.js (the \"backend\" version of Javascript). A good way to do this is to use node version manager: some installation instructions are available on the user-interface repository . Make sure that that you have set the UI correctly so it will read the schema from the right repository. This can be be set by modifying its reproschema-ui/src/config.js file (e.g see here ). Modify the githubSrc so that it points to the URL of the schema of your protocol: module.exports = { /* eslint-disable */ githubSrc: \"https://raw.githubusercontent.com/your-gtihub-account/your-repository/branch-name/folder/protcol_schema_filename\", banner: \"This is a test\", assetsPublicPath: \"/your-repository/\", backendServer: null, consent: true, }; Once you have done that you can launch the app with: npm install # Install the Javascript dependencies npm run serve # Run the development server locally Open your browser and go to localhost:8080","title":"Serving the app locally"},{"location":"80-how-to-contribute/","text":"How to contribute There are many ways in which you can contribute to this project. We have a list of milestones for the different features we would like to include in this project. Those milestones have several opened issues related to them: have a look through those issues to see if there is any of them where you think you can help. We are also track the progress to our different goals using some of the integrated kanban boards that github offers. Kanban boards : They are a great way to keep track of who is doing what, how the different tasks of your project are moving along... Suggestions If you are unsure where to start, maybe have a read through some of those sections about the project. It might give you ideas. the motivations of this project the different short and long term goals the general organization of the project Otherwise here are some suggestions of the different tasks that you can contribute to. If however you feel more interested to start hacking at some of the issues related to long other goals, feel free to do so. Spreadsheets A lot of the work requires to interact with the checklists in their spreadsheet format. The spreadsheets are hosted on this google drive folder and we try to keep a back-up in the csv folder . The MRI part is the more advanced at this moment but we are looking for people to help with the M/EEG part. The content of the spreadsheets and the work involved there is described in more details here . Boilerplate method sections We want to create boilerplate method sections corresponding to a single item or a set of items of the checklist to automate methods writing once the checklist is completed. There is a lot of work to do there in terms of writing the boilerplate text as well as automating the methods section generation. Boilerplate related issues listed here . Documentation If you don't understand something about the project, its goals, its implementation or how to use, then it's most likely that we did not do a good enough job at explaining and describing it. Get in touch and we can work together to improve our documentation. There are also several aspects of the documentation that need to be expanded. Conversion scripts We use some python code to convert the spreadsheets into the set of jsonld files that the user interface needs as inputs to display the checklists. This needs further improvement so if python is your jam, feel free to dive in. We are also using the Reproschema python package to validate the schemas created. This package should also in the long-term help automating the creation of schemas, so there are definitely some features waiting to be written there. User interface The user interface is Javascript based and uses the Vue framework and this also needs a lot of tweaking, so if Javascript is cup of tea: get in touch! Here are some of the issues related to the UI. Style guide Python We rely on black to automate the python code formatting. We have additional checks on Github for code style and quality. pep8 speaks for code style the awesome sourcery for code quality and refactoring suggestions. Markdown For markdown styling we rely on remark . We check for dead links using the markdown-link-check github action.","title":"How to contribute"},{"location":"80-how-to-contribute/#how-to-contribute","text":"There are many ways in which you can contribute to this project. We have a list of milestones for the different features we would like to include in this project. Those milestones have several opened issues related to them: have a look through those issues to see if there is any of them where you think you can help. We are also track the progress to our different goals using some of the integrated kanban boards that github offers. Kanban boards : They are a great way to keep track of who is doing what, how the different tasks of your project are moving along...","title":"How to contribute"},{"location":"80-how-to-contribute/#suggestions","text":"If you are unsure where to start, maybe have a read through some of those sections about the project. It might give you ideas. the motivations of this project the different short and long term goals the general organization of the project Otherwise here are some suggestions of the different tasks that you can contribute to. If however you feel more interested to start hacking at some of the issues related to long other goals, feel free to do so.","title":"Suggestions"},{"location":"80-how-to-contribute/#spreadsheets","text":"A lot of the work requires to interact with the checklists in their spreadsheet format. The spreadsheets are hosted on this google drive folder and we try to keep a back-up in the csv folder . The MRI part is the more advanced at this moment but we are looking for people to help with the M/EEG part. The content of the spreadsheets and the work involved there is described in more details here .","title":"Spreadsheets"},{"location":"80-how-to-contribute/#boilerplate-method-sections","text":"We want to create boilerplate method sections corresponding to a single item or a set of items of the checklist to automate methods writing once the checklist is completed. There is a lot of work to do there in terms of writing the boilerplate text as well as automating the methods section generation. Boilerplate related issues listed here .","title":"Boilerplate method sections"},{"location":"80-how-to-contribute/#documentation","text":"If you don't understand something about the project, its goals, its implementation or how to use, then it's most likely that we did not do a good enough job at explaining and describing it. Get in touch and we can work together to improve our documentation. There are also several aspects of the documentation that need to be expanded.","title":"Documentation"},{"location":"80-how-to-contribute/#conversion-scripts","text":"We use some python code to convert the spreadsheets into the set of jsonld files that the user interface needs as inputs to display the checklists. This needs further improvement so if python is your jam, feel free to dive in. We are also using the Reproschema python package to validate the schemas created. This package should also in the long-term help automating the creation of schemas, so there are definitely some features waiting to be written there.","title":"Conversion scripts"},{"location":"80-how-to-contribute/#user-interface","text":"The user interface is Javascript based and uses the Vue framework and this also needs a lot of tweaking, so if Javascript is cup of tea: get in touch! Here are some of the issues related to the UI.","title":"User interface"},{"location":"80-how-to-contribute/#style-guide","text":"","title":"Style guide"},{"location":"80-how-to-contribute/#python","text":"We rely on black to automate the python code formatting. We have additional checks on Github for code style and quality. pep8 speaks for code style the awesome sourcery for code quality and refactoring suggestions.","title":"Python"},{"location":"80-how-to-contribute/#markdown","text":"For markdown styling we rely on remark . We check for dead links using the markdown-link-check github action.","title":"Markdown"},{"location":"90-contributors/","text":"Contributors If you contributed to the COBIDAS Checklist and your name is not listed, please add it by adding your details in this spreadsheet and let us know about it: we'll take care of the rest. Make sure that you are also listed as a contributor on our OSF project . Sanu Ann Abraham Federico Adolfi Johannes Algermissen I am a PhD student at the Donders Institute at Radboud University, Nijmegen, the Netherlands. I work on human reinforcement learning and decision making using fMRI and EEG. With respect to open science, I am particularly interested in pre-registration and standardizing methods reports. James Bartlett R\u00e9mi Gau I am a post doc at the Universit\u00e9 Catholique de Louvain la Neuve in Belgium. I investigate on multisensory integration and mostly rely on high-resolution fMRI for my work. I also have a small thing for open-transparent-reproducible science. Satrajit Ghosh Cassandra Gould van Praag I am a postdoc at the University of Oxford Department of Psychiatry in the Psychopharmacology and Emotion Research Laboratory . I support (f)MRI research across the Oxford Health Biomedical Research Centre Experimental Medicine theme , where I help researchers design, execute and analyse awesome experimental medicine investigations. I am passionate about bringing inexperienced researchers into the world of programming and Open Research through excellent documentation and accessible resources. Ruud Hortensius Anisha Keshavan Mary Miedema David Moreau Tim Van Mourik Ilona Ruotsalainen I am PhD student at the University of Jyv\u00e4skyl\u00e4, Finland. Cristina Scarpazza Jeremy Simon Eduard Klapwijk Tom Nichols Zsuzsika Sjoerds Angie Tepper Martina G. Vilas Kristina Wiebels Dorien Huijser Wouter Weeda Contributions Below is a more precise breakdown of who contributed to what: Design, idea, planning Cassandra Gould van Praag R\u00e9mi Gau Ilona Ruotsalainen Angie Tepper Mary Miedema Martina G. Vilas Federico Adolfi Cristina Scarpazza Tom Nichols Spreadsheets MRI Cassandra Gould van Praag R\u00e9mi Gau Ilona Ruotsalainen Angie Tepper MEEG Mary Miedema Martina G. Vilas Federico Adolfi R\u00e9mi Gau Neurovault R\u00e9mi Gau Eyetracker R\u00e9mi Gau Reproschema Sanu Ann Abraham Satrajit Ghosh Anisha Keshavan R\u00e9mi Gau User interface Sanu Ann Abraham Federico Adolfi R\u00e9mi Gau Tim Van Mourik Anisha Keshavan Use case definition Meta-analysis Cristina Scarpazza David Moreau fMRI pre-registration Zsuzsika Sjoerds Kristina Wiebels R\u00e9mi Gau EEG / MEG pre-registration James Bartlett Ruud Hortensius Eduard Klapwijk Jeremy Simon BIDS compatibility Zsuzsika Sjoerds Kristina Wiebels R\u00e9mi Gau Boilerplate method section Wouter Weeda Dorien Huijser","title":"Contributors"},{"location":"90-contributors/#contributors","text":"If you contributed to the COBIDAS Checklist and your name is not listed, please add it by adding your details in this spreadsheet and let us know about it: we'll take care of the rest. Make sure that you are also listed as a contributor on our OSF project . Sanu Ann Abraham Federico Adolfi Johannes Algermissen I am a PhD student at the Donders Institute at Radboud University, Nijmegen, the Netherlands. I work on human reinforcement learning and decision making using fMRI and EEG. With respect to open science, I am particularly interested in pre-registration and standardizing methods reports. James Bartlett R\u00e9mi Gau I am a post doc at the Universit\u00e9 Catholique de Louvain la Neuve in Belgium. I investigate on multisensory integration and mostly rely on high-resolution fMRI for my work. I also have a small thing for open-transparent-reproducible science. Satrajit Ghosh Cassandra Gould van Praag I am a postdoc at the University of Oxford Department of Psychiatry in the Psychopharmacology and Emotion Research Laboratory . I support (f)MRI research across the Oxford Health Biomedical Research Centre Experimental Medicine theme , where I help researchers design, execute and analyse awesome experimental medicine investigations. I am passionate about bringing inexperienced researchers into the world of programming and Open Research through excellent documentation and accessible resources. Ruud Hortensius Anisha Keshavan Mary Miedema David Moreau Tim Van Mourik Ilona Ruotsalainen I am PhD student at the University of Jyv\u00e4skyl\u00e4, Finland. Cristina Scarpazza Jeremy Simon Eduard Klapwijk Tom Nichols Zsuzsika Sjoerds Angie Tepper Martina G. Vilas Kristina Wiebels Dorien Huijser Wouter Weeda","title":"Contributors"},{"location":"90-contributors/#contributions","text":"Below is a more precise breakdown of who contributed to what:","title":"Contributions"},{"location":"90-contributors/#design-idea-planning","text":"Cassandra Gould van Praag R\u00e9mi Gau Ilona Ruotsalainen Angie Tepper Mary Miedema Martina G. Vilas Federico Adolfi Cristina Scarpazza Tom Nichols","title":"Design, idea, planning"},{"location":"90-contributors/#spreadsheets","text":"","title":"Spreadsheets"},{"location":"90-contributors/#mri","text":"Cassandra Gould van Praag R\u00e9mi Gau Ilona Ruotsalainen Angie Tepper","title":"MRI"},{"location":"90-contributors/#meeg","text":"Mary Miedema Martina G. Vilas Federico Adolfi R\u00e9mi Gau","title":"MEEG"},{"location":"90-contributors/#neurovault","text":"R\u00e9mi Gau","title":"Neurovault"},{"location":"90-contributors/#eyetracker","text":"R\u00e9mi Gau","title":"Eyetracker"},{"location":"90-contributors/#reproschema","text":"Sanu Ann Abraham Satrajit Ghosh Anisha Keshavan R\u00e9mi Gau","title":"Reproschema"},{"location":"90-contributors/#user-interface","text":"Sanu Ann Abraham Federico Adolfi R\u00e9mi Gau Tim Van Mourik Anisha Keshavan","title":"User interface"},{"location":"90-contributors/#use-case-definition","text":"","title":"Use case definition"},{"location":"90-contributors/#meta-analysis","text":"Cristina Scarpazza David Moreau","title":"Meta-analysis"},{"location":"90-contributors/#fmri-pre-registration","text":"Zsuzsika Sjoerds Kristina Wiebels R\u00e9mi Gau","title":"fMRI pre-registration"},{"location":"90-contributors/#eeg-meg-pre-registration","text":"James Bartlett Ruud Hortensius Eduard Klapwijk Jeremy Simon","title":"EEG / MEG pre-registration"},{"location":"90-contributors/#bids-compatibility","text":"Zsuzsika Sjoerds Kristina Wiebels R\u00e9mi Gau","title":"BIDS compatibility"},{"location":"90-contributors/#boilerplate-method-section","text":"Wouter Weeda Dorien Huijser","title":"Boilerplate method section"},{"location":"99-references/","text":"References There is zotero group where we try to keep track the relevant literature related to this project. Let us know if we missed something. About the checklist website our project on the Open-Science Framework with the DOI:10.17605/OSF.IO/ANVQY is a where we try to centralize all the information across repos, google drive... a twitter thread The COBIDAS reports for MRI and fMRI for EEG and MEG MEEG report presentation at OHBM 2019 Jeanette Mumford has a 30 min video explaining the background behind the COBIDAS report and giving a run through of the checklist. Presentation slides made about this project can be found in the presentations folder . The original spreadsheet version of the COBIDAS checklist (thanks to Cass !!!) Related reporting guidelines Guideline for Reporting Standards of Eye-tracking Research in Decision Sciences Guidelines for the content and format of PET brain data in publications and archives: A consensus paper Related checklists A consensus-based transparency checklist and its shinyapp Checklist for Artifical Intelligence in Medical Imaging Consensus on the reporting and experimental design of clinical and cognitive-beharioural neurofeedback studies","title":"References"},{"location":"99-references/#references","text":"There is zotero group where we try to keep track the relevant literature related to this project. Let us know if we missed something.","title":"References"},{"location":"99-references/#about-the-checklist","text":"website our project on the Open-Science Framework with the DOI:10.17605/OSF.IO/ANVQY is a where we try to centralize all the information across repos, google drive... a twitter thread","title":"About the checklist"},{"location":"99-references/#the-cobidas-reports","text":"for MRI and fMRI for EEG and MEG MEEG report presentation at OHBM 2019 Jeanette Mumford has a 30 min video explaining the background behind the COBIDAS report and giving a run through of the checklist. Presentation slides made about this project can be found in the presentations folder . The original spreadsheet version of the COBIDAS checklist (thanks to Cass !!!)","title":"The COBIDAS reports"},{"location":"99-references/#related-reporting-guidelines","text":"Guideline for Reporting Standards of Eye-tracking Research in Decision Sciences Guidelines for the content and format of PET brain data in publications and archives: A consensus paper","title":"Related reporting guidelines"},{"location":"99-references/#related-checklists","text":"A consensus-based transparency checklist and its shinyapp Checklist for Artifical Intelligence in Medical Imaging Consensus on the reporting and experimental design of clinical and cognitive-beharioural neurofeedback studies","title":"Related checklists"}]}